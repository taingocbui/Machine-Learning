My team includes me, Tai Bui, and Caleb Joseph. Caleb is responsible to work with the theoretical part of the homework while I work with the coding problems. 

For my coding solutions, there are 2 classes, including TreeNode and DecisionTree, and several different functions, not including the given load function for each data set. 

The entropy function will calculates the entropy values given an array of frequency of each value in an attribute. Another param used in this fucntion is model which allows users to use either gini index or entropy formula for calculation.

The TreeNode class creates nodes for the decision tree with parameters including ids, depth, split_attribute, children, order and label. ids is a list of all index of data stored in the node. The depth param is the distance from this node to the root node. The split_attribute is the best_attribute generated after entropy calulation stored in this node. The children param is a list of all children belongs to this node. The order param stores a list of all unique values in the best_attribute feature, this param is used to locate the route for prediction when searching for labels later. Finally the label param is the label stored in the node if it is a leaf node (with no children). There are two other functions in this class. The set_properties function sets the split_attribute as well as the list of all unique values in the attribute to the node. The set_label will store the label to the node if it is a leaf node.

 The DecisionTree class uses GrowTree algorithm to grow the decision tree and assign each node with certain attribute or value (if leaf) accordingly. The fit function will set a root node which includes all ids and add the root to a queue. This queue will pop each element for each iteration, during each iteration, the function will split the data and assign values accordingly to both the node itself and their children. The split will continue until the len(split) is empty, then the node will be set a label as it does not have any children. When the tree max its size, the node will be also set with a label. The _entropy basically will obtain the frequency of each values given the node with all ids stored. Then, it will call the entropy function to obtain the entropy value for each value in a certain feature. The _split function will split the main data into sub_data based on all ids stored in a given node. Then it will store the best_attribute and a list of unique values of that attribute in the given node. This function also return a list of all children with each children contains a list of all ids splitted. The predict function will iterates along the new_data, with each row is a datapoint. Then the function will route the data points along the tree to obtain the label for each row in new_data. If the new_data's row contains a value which is not in the corresponding attribute in the tree, the tree will choose the first child node of that attribute node. Finally, the get_score will takes in the list of prediction and a list of target and compare each other to obtain the accuracy of our prediction.

For this homework, I used Jupyter notebook to code. 
Please let me know if you need me to make any adjustment. 
